+++
title = "VC Dimension, Uniform Convergence, and Sample Complexity in PAC Learning - II"
speaker = "Tushant Jha"
speakerhomepage = "https://in.linkedin.com/in/particlemania"
speakerinstitute = "CSTAR, IIITH"
speakerinstitutehomepage = "http://iiit.ac.in"
talkdate = 2019-07-24T16:00:00+09:00
venue = "A3 117 Conf. Hall, CSTAR Corridor"
meta = true
math = true
toc = false
categories = ["Learning Theory"]
abstract = """
How many samples do we need to observe before making a reliable prediction? This is the fundamental class of questions, called sample complexity, asked in learning theory.

In this talk, we shall review PAC Learning, an elementary and ubiquitous framework to talk about theoretical guarantees in statistics and machine learning. We shall then explore ideas from the 1971 landmark paper of Vladimir Vapnik and Alexey Chervonenkis, which proposed a combinatorial quantity (now called VC dimension) as a complexity measure for a class of binary classification models. We shall discuss how it characterizes learnability and uniform convergence, prove theorems for lower bound and upper bound of the sample complexity, and explore the relationship with empirical risk minimization algorithms.

If time allows, we shall also aim to look at, and prove corresponding results for, generalizations of VC dimension like: a) Pollard's pseudodimension (for regression), b) Natarajan dimension and Graph dimension (for multiclass classification), c) and Littlestone dimension (for online learning); which provide similar quantification in other learning frameworks.
"""
+++

*Prereq*: basic probability theory, and complexity-theoretic reductions

*Ambition*: to build an basic intuition of Sample Complexity 101.